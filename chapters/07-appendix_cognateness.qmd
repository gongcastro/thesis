
### Identifying cognates

Generally, a pair of words from two different languages that share meaning---translation equivalents---are considered as *cognates* if they share etymological origin (e.g., *table* and *taula*, in English and Catalan). The fact that two words share etymological origin frequently leads to them also being form-similar, as reflected by their overlapping orthographic or phonological form. This makes many cognates perceptually similar. In psycholinguistics, the term *cognateness* is often used to refer to the form-similarity that a pair of translation equivalents share, which has been found to impact how bilinguals process such word-forms [e.g., @costa2000cognate; @spivey1999cross]. Whether two form-similar translation equivalents are etymologically related or not is arguably tangential to the question of how cognateness affect language processing. For instance, the translation pair *much* and *mucho*, in English and Spanish have the same meaning, and are considerably similar at the orthographical and phonological level. Nonetheless, they come from different Proto-Indo-European roots [@campbell2007glossary]. This does not keep bilinguals from processing such kind of word-forms word forms differently than other translation equivalents with no form-similarity like 'dog' and *perro*, in English and Spanish. In the scope of this experiment, we consider any form-similar translation equivalent as a *cognate* pair, regardless of whether both word-forms share etymological origin.

In the present investigation, we capitalise on phonology. We operationalised *cognateness* as the phonological similarity between a pair of translation equivalents. Our dataset contained `r n_distinct(bvq_data$pool$te)` translation equivalents, each consisting in two word-forms: Catalan and Spanish (see @tbl-translation-levenshtein for an example). Measuring the phonological similarity between two word forms is non trial. While orthographic similarity can be calculated using string similarity/distance measures on the written word-forms, phonology poses additional challenges. A first, bottom-up approach to the task might be considering acoustic similarity as a proxy to phonological similarity. This may involve registering audio recordings of a talker reading each word-form aloud, and then comparing their spectrograms, cochleograms, or formant tracks [e.g., @heeringa2003norwegian]. While this is possible, this method computes acoustic similarity, as opposed to phonological similarity. This is not optimal, since a purely acoustic measure of similarity ignores how the actual signal is perceived by a actual listener. Since approximately 12 months of age, humans perceive the acoustic sounds in speech as phonemes, according to the phonology of their native language(s) [@werker1984crosslanguage; @kuhl1992linguistic]. This means that two listeners may not perceive the same linguistically relevant acoustic signal identically. This is why phonological similarity needs to be computed from discrete units that are meaningful for language perception.

A way to achieve this is to use phonological transcriptions of the word forms. Phonological transcriptions are symbolic, visual representations of the sounds in speech that capture some minimal  set of its features following some standard. The International Phonetic Alphabet (IPA) is one instance of such standards, providing a set of symbols that correspond to specific phonemes, as defined by their position across a series of dimensions. Using IPA transcriptions of word forms, one can use the same similarity/distance metric as in the case of orthographic word-forms. This method has been successfully used to measure the pairwise similarity between phonological word-forms from the  same language [e.g., @fourtassi2020growth] and across languages [@heeringa2004measuring; @floccia2018ii], and also between orthographic word-forms [@schepens2012distributions].

Multiple algorithms have been created to compute the distance between two strings of characters. The Levenshtein distance being one of them. The this algorithm calculates the minimum number of edit operations (additions, deletions, and substitutions) that one string must go through to become identical to the other string [@levenshtein1966binary] (see @eq-levenshtein). For instance, /\textipa{"ka.za}/ (*house*, in Catalan), would need to go through one substitution to become identical to its translation equivalent in Spanish /\textipa{"ka.sa}/ (/z/ is replaced by /s/). Therefore, the Levenshtein distance between /\textipa{"ka.za}/ and /\textipa{"ka.sa}/ is one. For an easier handling of non-ASCII characters (fairly prevalent in Catalan and Spanish, e.g., á, ü, ñ), we used phonological transcriptions in X-SAMPA format, as opposed to IPA, for its computer-friendly set of symbols [@wells1995computercoding].

$$
\text{lev}(x, y) =  
\begin{cases}
\max(i, j) & \text{if} \ \min(i, j)=0\\
\min \begin{cases}
\ \text{lev}_{x, y}(i-1, j) + 1 \\
\ \text{lev}_{x, y}(i, j-1) + 1 \\
\ \text{lev}_{x, y}(i-1, j-1) + 1_{a_i \neq b_j} \\
\end{cases}
\ &\text{otherwise}
\end{cases}
$${#eq-levenshtein}

Here, $a$ and $b$ are the character strings corresponding to the phonological transcriptions of two word-forms belonging to the same translation equivalent, where each character corresponds to one phoneme expressed as a symbol from the SAMPA alphabet. $i$ and $j$ are the length (i.e., number of phonemes) of the $s$ and $t$ strings respectively. 

To measure the phonological similarity between the translation equivalents in our study, we computed a normalised versions of the Levenshtein distance between each pair of word-forms that accounts for the length of the strings. The rationale behind this correction is that longer word-forms are more likely to differ from their counterpart, compared to shorter word forms, and that this tendency does not correspond necessarily to a larger distance in terms of perception. This normalisation is achieved by dividing the Levenshtein distance by the length of the longest string, which leads to a distance metric that ranges from 0 to 1. This can be interpreted as a proportion of characters in the longest string that need to be edited in order for that string to become identical to the shorter string. A 0% normalised Levenshtein distance indicates that both word-forms are identical. A 50% normalised Levenshtein distance indicates that half of the phonemes in the longest word-form must be edited for both word-forms to become identical. A 100% normalised Levenshtein distance indicates that the two word-forms are completely different, as *all* phonemes in the longest word-form must be changed for it to become identical to the other.

Finally, since we are interested in *cognateness*, in order to easy the interpretation of the analyses we subtracted the normalised Levenshtein distance from one, so that the metric indicated the *similarity* between the each pair of word-forms, instead of their distance. @eq-levenshtein-similarity shows the formula of this metric.

$$
1-\frac{\text{lev}(a, b)}{\max{\{i, j}\}}
$${#eq-levenshtein-similarity}

The R package `stringdist` [@vanderloo2014stringdist] offers the `stringsim()` function to compute this normalised Levenshtein similarity measure, see @tbl-translation-levenshtein for more examples:

```{r tbl-translation-levenshtein}
#| label: tbl-translation-levenshtein
#| tbl-cap: Normalised Levenshtein similarity computed for three exemplars of translation pairs in our study
#| echo: false
tribble(
	~Catalan, ~Spanish, ~Levenshtein,
	"porta /pOrt5/", "puerta /pwe4ta/", 0.50,
	"taula /tawl5/", "mesa /mesa/", 0.00,
	"cotxe /kOtS5/", "coche /kotSe/", 0.40
) |>
	gt() |>
	fmt_number(is.numeric)
```